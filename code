# Import RegexpTokenizer from nltk.tokenize module
# RegexpTokenizer is used to split text into tokens (words) based on a regular expression pattern
from nltk.tokenize import RegexpTokenizer

# Import extract_text from pdfminer.high_level module
# extract_text() is used to extract raw text content from a PDF file
from pdfminer.high_level import extract_text

# Import FreqDist from nltk.probability module
# FreqDist is used to calculate the frequency distribution of tokens (counts occurrences)
from nltk.probability import FreqDist

# Extract the text from a PDF file
# r before the string is a raw string literal (to handle backslashes in file path correctly)
text = extract_text(r"C:\Users\91984\Downloads\ISR Unit 1.pdf")

# Print the first 50 characters of extracted text to check if extraction worked
print("Extracted Text Sample:\n", text[:50])

# Create an instance of RegexpTokenizer with the pattern '\w+'
# \w+ means match sequences of alphanumeric characters (a-z, A-Z, 0-9) and underscore (_)
# It ignores punctuation marks like .,;:'" etc.
tokenizer = RegexpTokenizer('\w+')

# Tokenize the extracted text using the tokenizer
# It returns a list of words (tokens) from the text
tokens = tokenizer.tokenize(text)

# Print the total number of tokens extracted
print("\nTotal Tokens Extracted:", len(tokens))

# Print the first 20 tokens as a sample to verify tokenization
print("Sample Tokens:", tokens[:20])

# Create a frequency distribution of the tokens
# freqdist is a dictionary-like object where keys = tokens, values = their frequency counts
freqdist = FreqDist(tokens)

# Print the 10 most common words along with their frequencies
print("\nMost Common 10 Words:", freqdist.most_common(10))

# Create a list of unique words (using set(tokens)) where:
# - word length is greater than 5 characters (len(word) > 5)
# - frequency of occurrence is greater than 20 (freqdist[word] > 20)
long_frequent_words = [word for word in set(tokens) if len(word) > 5 and freqdist[word] > 20]

# Print all words that satisfy the above condition
print("\nWords with length > 5 and frequency > 20:")
print(long_frequent_words)

# Print the total number of words that matched the above criteria
print("\nTotal Number of Long Frequent Words:", len(long_frequent_words))
